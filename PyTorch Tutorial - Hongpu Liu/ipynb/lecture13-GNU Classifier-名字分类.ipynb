{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN(GNU) Classifier - 名字分类\n",
    "![N6oFWA](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/N6oFWA.png)\n",
    "- 训练一个RNN模型来将名字分类为对应的国家\n",
    "    - 输入：一个字符串序列（名字）\n",
    "    - 输出：\n",
    "    - **注意：输入序列的长短是不一致的！**\n",
    "\n",
    "![3LEwHw](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/3LEwHw.png)\n",
    "![mfafrX](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/mfafrX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import Dataset\n",
    "import gzip\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100 # 隐层数量\n",
    "BATCH_SIZE = 256 \n",
    "N_LAYER = 2 # GRU层数\n",
    "N_EPOCHS = 100 \n",
    "N_CHARS = 128 \n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出训练时间\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "# 在创建tensor时判断是否要用GPU\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "![PWjlFL](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/PWjlFL.png)\n",
    "![yRc9Ve](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/yRc9Ve.png)\n",
    "![0sEDlv](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/0sEDlv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'dataset/names/names_train.csv.gz' if is_train_set else 'dataset/names/names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            reader = csv.reader(f) \n",
    "            rows = list(reader) # 将读取到的文件内容转换为list\n",
    "        self.names = [row[0] for row in rows] # 使用列表生成式生成名字list\n",
    "        self.len = len(self.names) \n",
    "        self.countries = [row[1] for row in rows] # 使用列表生成式生成国籍list\n",
    "        self.country_list = list(sorted(set(self.countries))) # 得到了去重、排序之后的国籍list\n",
    "        self.country_dict = self.getCountryDict() # 构造国籍的词典\n",
    "        self.country_num = len(self.country_list)\n",
    "        \n",
    "    # 使用索引得到的数据是 (姓名，国籍索引)\n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # 构造国籍字典\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict() \n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx \n",
    "        return country_dict\n",
    "    \n",
    "    # 通过索引查询国籍名\n",
    "    def idx2country(self, index):\n",
    "        return self.country_list[index]\n",
    "    \n",
    "    # 获得样本中的国籍数目\n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "testset = NameDataset(is_train_set=False) \n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainset.getCountriesNum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将名字转换为tensor的步骤\n",
    "![dHPlJQ](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/dHPlJQ.png)\n",
    "![mrqnKi](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/mrqnKi.png)\n",
    "![7ACDVc](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/7ACDVc.png)\n",
    "![uE7PXE](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/uE7PXE.png)\n",
    "![XSgTV7](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/XSgTV7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将名字转换为ascii列表\n",
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name] # ord()返回对应的 ASCII 数值，或者 Unicode 数值\n",
    "    return arr, len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转为tensor\n",
    "def make_tensors(names, countries):\n",
    "    sequences_and_lengths = [name2list(name) for name in names] \n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths] # name_sequences是姓名字符串转为的ASCII列表\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths]) \n",
    "    countries = countries.long()\n",
    "\n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long() # 先做一个全零的tensor\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0): # 将输入复制过去\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True) # 按序列长度进行排序\n",
    "    # 输入序列和对应的国家都要按照这个顺序排序\n",
    "    seq_tensor = seq_tensor[perm_idx] \n",
    "    countries = countries[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__() \n",
    "        self.hidden_size = hidden_size \n",
    "        self.n_layers = n_layers # GRU layers\n",
    "        self.n_directions = 2 if bidirectional else 1 # 如果是双向的则为2\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size) \n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, \n",
    "                                bidirectional=bidirectional) # bidirectional单向还是双向\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size) # hidden_size * self.n_directions表示拼接\n",
    "\n",
    "    def _init_hidden(self, batch_size): # 创建全零的初始隐层\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, \n",
    "                             batch_size, self.hidden_size) \n",
    "        return create_tensor(hidden)\n",
    "   \n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape : B x S -> S x B\n",
    "        input = input.t() # 转置transpose,把padding之后的数据变为竖的\n",
    "        batch_size = input.size(1)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size) \n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        ## RNN，LSTM，GRU都可以接受pack_padded_sequence过的数据作为输入\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths)\n",
    "\n",
    "        output, hidden = self.gru(gru_input, hidden) \n",
    "        if self.n_directions == 2:\n",
    "            # 拼接两个隐藏的状态\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1) \n",
    "        else:\n",
    "            hidden_cat = hidden[-1] \n",
    "        fc_output = self.fc(hidden_cat) \n",
    "        return fc_output\n",
    "\n",
    "classifier = RNNClassifier(N_CHARS , HIDDEN_SIZE, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Bi-Direction RNN/LSTM/GRU?\n",
    "![IrYpso](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/IrYpso.png)\n",
    "- 拼接正向和反向的隐藏层输出\n",
    "最终输出的隐层输出如下：\n",
    "![zGX0we](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/zGX0we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要转置？\n",
    "![gYUUtX](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/gYUUtX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pack_padded_sequence(embedding, seq_lengths)的作用\n",
    "![ZeXgqU](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/ZeXgqU.png)\n",
    "- pack_padded_sequence(embedding, seq_lengths)：\n",
    "    - The first parameter with shape:（𝑠𝑒𝑞𝐿𝑒𝑛, 𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒, ℎ𝑖𝑑𝑑𝑒𝑛𝑆𝑖𝑧𝑒）\n",
    "    - The second parameter is a tensor, which is a list of sequence length of each batch element.\n",
    "    - It returns a PackedSquence object.\n",
    "\n",
    "![fpMKUf](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/fpMKUf.png)\n",
    "![wUM7n1](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/wUM7n1.png)\n",
    "![htalH6](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/htalH6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries) \n",
    "        output = classifier(inputs, seq_lengths) \n",
    "        loss = criterion(output, target) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() \n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch} ', end='') \n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='') \n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    correct = 0 \n",
    "    total = len(testset) \n",
    "    print(\"evaluating trained model ...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries) \n",
    "            output = classifier(inputs, seq_lengths) \n",
    "            pred = output.max(dim=1, keepdim=True)[1] \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        percent = '%.2f' % (100 * correct / total) \n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[0m 0s] Epoch 1 [2560/13374] loss=0.008831825572997332\n",
      "[0m 0s] Epoch 1 [5120/13374] loss=0.007599199074320495\n",
      "[0m 0s] Epoch 1 [7680/13374] loss=0.006925769538308184\n",
      "[0m 0s] Epoch 1 [10240/13374] loss=0.00644455156289041\n",
      "[0m 0s] Epoch 1 [12800/13374] loss=0.006090904558077454\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4443/6700 66.31%\n",
      "[0m 1s] Epoch 2 [2560/13374] loss=0.0042732032015919685\n",
      "[0m 1s] Epoch 2 [5120/13374] loss=0.00411197041394189\n",
      "[0m 1s] Epoch 2 [7680/13374] loss=0.003994902479462326\n",
      "[0m 1s] Epoch 2 [10240/13374] loss=0.0039050370862241833\n",
      "[0m 1s] Epoch 2 [12800/13374] loss=0.0038240877352654933\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4909/6700 73.27%\n",
      "[0m 2s] Epoch 3 [2560/13374] loss=0.0033062070375308394\n",
      "[0m 2s] Epoch 3 [5120/13374] loss=0.003286694409325719\n",
      "[0m 2s] Epoch 3 [7680/13374] loss=0.003147212532348931\n",
      "[0m 2s] Epoch 3 [10240/13374] loss=0.0030934260110370817\n",
      "[0m 2s] Epoch 3 [12800/13374] loss=0.0030627458496019244\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5200/6700 77.61%\n",
      "[0m 3s] Epoch 4 [2560/13374] loss=0.0026616920018568634\n",
      "[0m 3s] Epoch 4 [5120/13374] loss=0.00262841657968238\n",
      "[0m 3s] Epoch 4 [7680/13374] loss=0.0025922990093628566\n",
      "[0m 3s] Epoch 4 [10240/13374] loss=0.002596757508581504\n",
      "[0m 3s] Epoch 4 [12800/13374] loss=0.0025541685475036504\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5372/6700 80.18%\n",
      "[0m 4s] Epoch 5 [2560/13374] loss=0.0021870323922485112\n",
      "[0m 4s] Epoch 5 [5120/13374] loss=0.00219451094744727\n",
      "[0m 4s] Epoch 5 [7680/13374] loss=0.0022131812060251833\n",
      "[0m 4s] Epoch 5 [10240/13374] loss=0.0022355988912750036\n",
      "[0m 4s] Epoch 5 [12800/13374] loss=0.002227674121968448\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5439/6700 81.18%\n",
      "[0m 5s] Epoch 6 [2560/13374] loss=0.0021544003626331686\n",
      "[0m 5s] Epoch 6 [5120/13374] loss=0.002034695789916441\n",
      "[0m 5s] Epoch 6 [7680/13374] loss=0.002018135320395231\n",
      "[0m 5s] Epoch 6 [10240/13374] loss=0.001997441265848465\n",
      "[0m 5s] Epoch 6 [12800/13374] loss=0.001990736161824316\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5539/6700 82.67%\n",
      "[0m 6s] Epoch 7 [2560/13374] loss=0.0018108763615600765\n",
      "[0m 6s] Epoch 7 [5120/13374] loss=0.001823824329767376\n",
      "[0m 6s] Epoch 7 [7680/13374] loss=0.0018025190841096143\n",
      "[0m 6s] Epoch 7 [10240/13374] loss=0.00181096694432199\n",
      "[0m 6s] Epoch 7 [12800/13374] loss=0.0018046135501936077\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5550/6700 82.84%\n",
      "[0m 7s] Epoch 8 [2560/13374] loss=0.0016140077845193447\n",
      "[0m 7s] Epoch 8 [5120/13374] loss=0.0015754032356198877\n",
      "[0m 7s] Epoch 8 [7680/13374] loss=0.0015719559354086716\n",
      "[0m 7s] Epoch 8 [10240/13374] loss=0.0016227564250584693\n",
      "[0m 7s] Epoch 8 [12800/13374] loss=0.0016197372879832982\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5557/6700 82.94%\n",
      "[0m 8s] Epoch 9 [2560/13374] loss=0.0015147742233239114\n",
      "[0m 8s] Epoch 9 [5120/13374] loss=0.0015035420889034866\n",
      "[0m 8s] Epoch 9 [7680/13374] loss=0.0015205672631661098\n",
      "[0m 8s] Epoch 9 [10240/13374] loss=0.0015142372605623678\n",
      "[0m 8s] Epoch 9 [12800/13374] loss=0.001500119974371046\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5629/6700 84.01%\n",
      "[0m 9s] Epoch 10 [2560/13374] loss=0.0013150132494047285\n",
      "[0m 9s] Epoch 10 [5120/13374] loss=0.001346776622813195\n",
      "[0m 9s] Epoch 10 [7680/13374] loss=0.0013612320685448746\n",
      "[0m 9s] Epoch 10 [10240/13374] loss=0.0013674068759428337\n",
      "[0m 9s] Epoch 10 [12800/13374] loss=0.0013725612475536763\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[0m 10s] Epoch 11 [2560/13374] loss=0.0011777572333812713\n",
      "[0m 10s] Epoch 11 [5120/13374] loss=0.0011379437579307704\n",
      "[0m 10s] Epoch 11 [7680/13374] loss=0.001179588910114641\n",
      "[0m 10s] Epoch 11 [10240/13374] loss=0.001225004361185711\n",
      "[0m 10s] Epoch 11 [12800/13374] loss=0.0012299513292964549\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5646/6700 84.27%\n",
      "[0m 11s] Epoch 12 [2560/13374] loss=0.0010856789536774158\n",
      "[0m 11s] Epoch 12 [5120/13374] loss=0.0010829832463059574\n",
      "[0m 11s] Epoch 12 [7680/13374] loss=0.00108403490545849\n",
      "[0m 11s] Epoch 12 [10240/13374] loss=0.0010973597367410549\n",
      "[0m 11s] Epoch 12 [12800/13374] loss=0.0010989209124818444\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[0m 12s] Epoch 13 [2560/13374] loss=0.0009614724083803594\n",
      "[0m 12s] Epoch 13 [5120/13374] loss=0.000918279992765747\n",
      "[0m 12s] Epoch 13 [7680/13374] loss=0.0009520976066899797\n",
      "[0m 12s] Epoch 13 [10240/13374] loss=0.0009540475875837728\n",
      "[0m 12s] Epoch 13 [12800/13374] loss=0.0009877103730104864\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5657/6700 84.43%\n",
      "[0m 13s] Epoch 14 [2560/13374] loss=0.0008994831005111337\n",
      "[0m 13s] Epoch 14 [5120/13374] loss=0.0008861061709467321\n",
      "[0m 13s] Epoch 14 [7680/13374] loss=0.000900826669142892\n",
      "[0m 13s] Epoch 14 [10240/13374] loss=0.0009113771448028274\n",
      "[0m 13s] Epoch 14 [12800/13374] loss=0.000910271282773465\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5677/6700 84.73%\n",
      "[0m 14s] Epoch 15 [2560/13374] loss=0.0007837238750653342\n",
      "[0m 14s] Epoch 15 [5120/13374] loss=0.0007616378352395259\n",
      "[0m 14s] Epoch 15 [7680/13374] loss=0.0007691316490915294\n",
      "[0m 14s] Epoch 15 [10240/13374] loss=0.0007845623513276223\n",
      "[0m 14s] Epoch 15 [12800/13374] loss=0.0007825087109813466\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5640/6700 84.18%\n",
      "[0m 15s] Epoch 16 [2560/13374] loss=0.0006356825702823699\n",
      "[0m 15s] Epoch 16 [5120/13374] loss=0.0006540280563058331\n",
      "[0m 15s] Epoch 16 [7680/13374] loss=0.0006797845106727133\n",
      "[0m 15s] Epoch 16 [10240/13374] loss=0.0006759328607586212\n",
      "[0m 15s] Epoch 16 [12800/13374] loss=0.0006866145529784262\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5657/6700 84.43%\n",
      "[0m 16s] Epoch 17 [2560/13374] loss=0.0006214487133547663\n",
      "[0m 16s] Epoch 17 [5120/13374] loss=0.0006161061814054847\n",
      "[0m 16s] Epoch 17 [7680/13374] loss=0.0006209826775981734\n",
      "[0m 16s] Epoch 17 [10240/13374] loss=0.0006201523705385625\n",
      "[0m 16s] Epoch 17 [12800/13374] loss=0.0006227742094779387\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5654/6700 84.39%\n",
      "[0m 17s] Epoch 18 [2560/13374] loss=0.0005284670332912356\n",
      "[0m 17s] Epoch 18 [5120/13374] loss=0.000566025149601046\n",
      "[0m 17s] Epoch 18 [7680/13374] loss=0.0005585243062038596\n",
      "[0m 17s] Epoch 18 [10240/13374] loss=0.0005596988943580072\n",
      "[0m 17s] Epoch 18 [12800/13374] loss=0.0005601658776868135\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5643/6700 84.22%\n",
      "[0m 18s] Epoch 19 [2560/13374] loss=0.0004636804747860879\n",
      "[0m 18s] Epoch 19 [5120/13374] loss=0.0005003195634344593\n",
      "[0m 18s] Epoch 19 [7680/13374] loss=0.0004975111864041537\n",
      "[0m 18s] Epoch 19 [10240/13374] loss=0.0004975296331394929\n",
      "[0m 18s] Epoch 19 [12800/13374] loss=0.0005133013537852093\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[0m 19s] Epoch 20 [2560/13374] loss=0.00042660019535105674\n",
      "[0m 19s] Epoch 20 [5120/13374] loss=0.000403886464482639\n",
      "[0m 19s] Epoch 20 [7680/13374] loss=0.00042541984100049983\n",
      "[0m 19s] Epoch 20 [10240/13374] loss=0.0004410272827954032\n",
      "[0m 20s] Epoch 20 [12800/13374] loss=0.00045341572316829113\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[0m 20s] Epoch 21 [2560/13374] loss=0.000348497205413878\n",
      "[0m 20s] Epoch 21 [5120/13374] loss=0.0003799490470555611\n",
      "[0m 20s] Epoch 21 [7680/13374] loss=0.0003802581408914799\n",
      "[0m 20s] Epoch 21 [10240/13374] loss=0.00038880418360349723\n",
      "[0m 20s] Epoch 21 [12800/13374] loss=0.0004036205989541486\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5646/6700 84.27%\n",
      "[0m 21s] Epoch 22 [2560/13374] loss=0.00032682177261449396\n",
      "[0m 21s] Epoch 22 [5120/13374] loss=0.00034090000408468766\n",
      "[0m 21s] Epoch 22 [7680/13374] loss=0.00033761972833114365\n",
      "[0m 21s] Epoch 22 [10240/13374] loss=0.0003497408208204433\n",
      "[0m 21s] Epoch 22 [12800/13374] loss=0.0003698042512405664\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5658/6700 84.45%\n",
      "[0m 22s] Epoch 23 [2560/13374] loss=0.00032850171555764973\n",
      "[0m 22s] Epoch 23 [5120/13374] loss=0.00033076400140998887\n",
      "[0m 22s] Epoch 23 [7680/13374] loss=0.0003398659435333684\n",
      "[0m 22s] Epoch 23 [10240/13374] loss=0.0003481170322629623\n",
      "[0m 22s] Epoch 23 [12800/13374] loss=0.000347706179600209\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[0m 23s] Epoch 24 [2560/13374] loss=0.00028875450807390733\n",
      "[0m 23s] Epoch 24 [5120/13374] loss=0.00029496106508304366\n",
      "[0m 23s] Epoch 24 [7680/13374] loss=0.00030620802426710724\n",
      "[0m 23s] Epoch 24 [10240/13374] loss=0.0003132154241029639\n",
      "[0m 23s] Epoch 24 [12800/13374] loss=0.00033033284009434285\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5653/6700 84.37%\n",
      "[0m 24s] Epoch 25 [2560/13374] loss=0.0002828162134392187\n",
      "[0m 24s] Epoch 25 [5120/13374] loss=0.0002765071614703629\n",
      "[0m 24s] Epoch 25 [7680/13374] loss=0.0002725687760782118\n",
      "[0m 24s] Epoch 25 [10240/13374] loss=0.00028522589927888476\n",
      "[0m 24s] Epoch 25 [12800/13374] loss=0.0003007856564363465\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[0m 25s] Epoch 26 [2560/13374] loss=0.0002655485368450172\n",
      "[0m 25s] Epoch 26 [5120/13374] loss=0.0002823270348017104\n",
      "[0m 25s] Epoch 26 [7680/13374] loss=0.00027529461513040585\n",
      "[0m 25s] Epoch 26 [10240/13374] loss=0.0002920988557889359\n",
      "[0m 25s] Epoch 26 [12800/13374] loss=0.00029868611454730853\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5625/6700 83.96%\n",
      "[0m 26s] Epoch 27 [2560/13374] loss=0.00022564093524124473\n",
      "[0m 26s] Epoch 27 [5120/13374] loss=0.0002438200233882526\n",
      "[0m 26s] Epoch 27 [7680/13374] loss=0.0002590314002494173\n",
      "[0m 26s] Epoch 27 [10240/13374] loss=0.00026646125170373127\n",
      "[0m 26s] Epoch 27 [12800/13374] loss=0.00026786800866830164\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5633/6700 84.07%\n",
      "[0m 27s] Epoch 28 [2560/13374] loss=0.0002556321080191992\n",
      "[0m 27s] Epoch 28 [5120/13374] loss=0.0002512601226044353\n",
      "[0m 27s] Epoch 28 [7680/13374] loss=0.0002472569191013463\n",
      "[0m 27s] Epoch 28 [10240/13374] loss=0.00026410872596898115\n",
      "[0m 28s] Epoch 28 [12800/13374] loss=0.0002696757824742235\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[0m 28s] Epoch 29 [2560/13374] loss=0.00021330775489332155\n",
      "[0m 28s] Epoch 29 [5120/13374] loss=0.00021926800400251523\n",
      "[0m 28s] Epoch 29 [7680/13374] loss=0.00023596757576645663\n",
      "[0m 28s] Epoch 29 [10240/13374] loss=0.00024230220515164547\n",
      "[0m 29s] Epoch 29 [12800/13374] loss=0.00025249544269172474\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5639/6700 84.16%\n",
      "[0m 29s] Epoch 30 [2560/13374] loss=0.00021719297365052626\n",
      "[0m 29s] Epoch 30 [5120/13374] loss=0.0002201618699473329\n",
      "[0m 29s] Epoch 30 [7680/13374] loss=0.00022960182250244542\n",
      "[0m 29s] Epoch 30 [10240/13374] loss=0.0002349149639485404\n",
      "[0m 30s] Epoch 30 [12800/13374] loss=0.00025166419247398154\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[0m 30s] Epoch 31 [2560/13374] loss=0.0002124584061675705\n",
      "[0m 30s] Epoch 31 [5120/13374] loss=0.00021158189483685418\n",
      "[0m 30s] Epoch 31 [7680/13374] loss=0.00022510281414724886\n",
      "[0m 30s] Epoch 31 [10240/13374] loss=0.00023427819505741354\n",
      "[0m 31s] Epoch 31 [12800/13374] loss=0.00024211837182519958\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[0m 31s] Epoch 32 [2560/13374] loss=0.0001786738110240549\n",
      "[0m 31s] Epoch 32 [5120/13374] loss=0.0002020703068410512\n",
      "[0m 31s] Epoch 32 [7680/13374] loss=0.0002185521055556213\n",
      "[0m 31s] Epoch 32 [10240/13374] loss=0.00022728663352609147\n",
      "[0m 31s] Epoch 32 [12800/13374] loss=0.00023803122836397962\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[0m 32s] Epoch 33 [2560/13374] loss=0.00021455811656778678\n",
      "[0m 32s] Epoch 33 [5120/13374] loss=0.00021666359971277415\n",
      "[0m 32s] Epoch 33 [7680/13374] loss=0.00022493298990108694\n",
      "[0m 32s] Epoch 33 [10240/13374] loss=0.0002211753439041786\n",
      "[0m 32s] Epoch 33 [12800/13374] loss=0.0002302514878101647\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[0m 33s] Epoch 34 [2560/13374] loss=0.00019118386626360007\n",
      "[0m 33s] Epoch 34 [5120/13374] loss=0.00021536801104957704\n",
      "[0m 33s] Epoch 34 [7680/13374] loss=0.00021308073943752484\n",
      "[0m 33s] Epoch 34 [10240/13374] loss=0.00021532936625590082\n",
      "[0m 33s] Epoch 34 [12800/13374] loss=0.00022931173705728725\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[0m 34s] Epoch 35 [2560/13374] loss=0.0002051346004009247\n",
      "[0m 34s] Epoch 35 [5120/13374] loss=0.00019776208209805192\n",
      "[0m 34s] Epoch 35 [7680/13374] loss=0.00020903981640003623\n",
      "[0m 34s] Epoch 35 [10240/13374] loss=0.00021305857026163722\n",
      "[0m 34s] Epoch 35 [12800/13374] loss=0.00022102605274994857\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[0m 35s] Epoch 36 [2560/13374] loss=0.00018623961659613997\n",
      "[0m 35s] Epoch 36 [5120/13374] loss=0.0001994003469008021\n",
      "[0m 35s] Epoch 36 [7680/13374] loss=0.0001932218639315882\n",
      "[0m 35s] Epoch 36 [10240/13374] loss=0.00020343587129900698\n",
      "[0m 35s] Epoch 36 [12800/13374] loss=0.00021713699054089374\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5625/6700 83.96%\n",
      "[0m 36s] Epoch 37 [2560/13374] loss=0.00018866752943722532\n",
      "[0m 36s] Epoch 37 [5120/13374] loss=0.0002057812780549284\n",
      "[0m 36s] Epoch 37 [7680/13374] loss=0.0002095406530618978\n",
      "[0m 36s] Epoch 37 [10240/13374] loss=0.00021048929047537968\n",
      "[0m 36s] Epoch 37 [12800/13374] loss=0.00021128901280462742\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[0m 37s] Epoch 38 [2560/13374] loss=0.00017537159292260184\n",
      "[0m 37s] Epoch 38 [5120/13374] loss=0.00018243840167997405\n",
      "[0m 37s] Epoch 38 [7680/13374] loss=0.00018935281404992567\n",
      "[0m 37s] Epoch 38 [10240/13374] loss=0.00018843969955923968\n",
      "[0m 37s] Epoch 38 [12800/13374] loss=0.0002061111436341889\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[0m 38s] Epoch 39 [2560/13374] loss=0.00016177168145077304\n",
      "[0m 38s] Epoch 39 [5120/13374] loss=0.00017664747247181367\n",
      "[0m 38s] Epoch 39 [7680/13374] loss=0.00018626096401324805\n",
      "[0m 38s] Epoch 39 [10240/13374] loss=0.00019407702293392504\n",
      "[0m 38s] Epoch 39 [12800/13374] loss=0.00020137952757067978\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5626/6700 83.97%\n",
      "[0m 39s] Epoch 40 [2560/13374] loss=0.00016681725974194707\n",
      "[0m 39s] Epoch 40 [5120/13374] loss=0.00017437956848880275\n",
      "[0m 39s] Epoch 40 [7680/13374] loss=0.00018010917944290364\n",
      "[0m 39s] Epoch 40 [10240/13374] loss=0.00018967632295243674\n",
      "[0m 39s] Epoch 40 [12800/13374] loss=0.0001985403236176353\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[0m 40s] Epoch 41 [2560/13374] loss=0.00017735216679284349\n",
      "[0m 40s] Epoch 41 [5120/13374] loss=0.0001798274195607519\n",
      "[0m 40s] Epoch 41 [7680/13374] loss=0.00018154631810223993\n",
      "[0m 40s] Epoch 41 [10240/13374] loss=0.0001863954708824167\n",
      "[0m 40s] Epoch 41 [12800/13374] loss=0.00019838289765175432\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[0m 41s] Epoch 42 [2560/13374] loss=0.00016459518519695848\n",
      "[0m 41s] Epoch 42 [5120/13374] loss=0.00017182895098812878\n",
      "[0m 41s] Epoch 42 [7680/13374] loss=0.0001792237118934281\n",
      "[0m 41s] Epoch 42 [10240/13374] loss=0.0001846659266448114\n",
      "[0m 41s] Epoch 42 [12800/13374] loss=0.0001973889282089658\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[0m 42s] Epoch 43 [2560/13374] loss=0.00014160685095703228\n",
      "[0m 42s] Epoch 43 [5120/13374] loss=0.00015891033872321713\n",
      "[0m 42s] Epoch 43 [7680/13374] loss=0.0001821062554275462\n",
      "[0m 42s] Epoch 43 [10240/13374] loss=0.00018650105830602114\n",
      "[0m 42s] Epoch 43 [12800/13374] loss=0.00020250289890100248\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[0m 43s] Epoch 44 [2560/13374] loss=0.0001469219452701509\n",
      "[0m 43s] Epoch 44 [5120/13374] loss=0.00016773482529970353\n",
      "[0m 43s] Epoch 44 [7680/13374] loss=0.0001744622083303208\n",
      "[0m 43s] Epoch 44 [10240/13374] loss=0.00018843943762476557\n",
      "[0m 43s] Epoch 44 [12800/13374] loss=0.0001939952769316733\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[0m 44s] Epoch 45 [2560/13374] loss=0.00016223399215959945\n",
      "[0m 44s] Epoch 45 [5120/13374] loss=0.00017315594523097387\n",
      "[0m 44s] Epoch 45 [7680/13374] loss=0.00018733043325482867\n",
      "[0m 44s] Epoch 45 [10240/13374] loss=0.0001976487294086837\n",
      "[0m 44s] Epoch 45 [12800/13374] loss=0.0002023763279430568\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5625/6700 83.96%\n",
      "[0m 45s] Epoch 46 [2560/13374] loss=0.00012161804988863878\n",
      "[0m 45s] Epoch 46 [5120/13374] loss=0.0001448176753910957\n",
      "[0m 45s] Epoch 46 [7680/13374] loss=0.00016020716260148522\n",
      "[0m 45s] Epoch 46 [10240/13374] loss=0.0001738808774462086\n",
      "[0m 45s] Epoch 46 [12800/13374] loss=0.00019093132330453956\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[0m 46s] Epoch 47 [2560/13374] loss=0.00015120864918571897\n",
      "[0m 46s] Epoch 47 [5120/13374] loss=0.00016275370107905475\n",
      "[0m 46s] Epoch 47 [7680/13374] loss=0.00017260871124259817\n",
      "[0m 46s] Epoch 47 [10240/13374] loss=0.0001839451449995977\n",
      "[0m 46s] Epoch 47 [12800/13374] loss=0.00019038032100070267\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[0m 47s] Epoch 48 [2560/13374] loss=0.0001450797622965183\n",
      "[0m 47s] Epoch 48 [5120/13374] loss=0.0001489654168835841\n",
      "[0m 47s] Epoch 48 [7680/13374] loss=0.00016847089476262528\n",
      "[0m 47s] Epoch 48 [10240/13374] loss=0.00017583012377144768\n",
      "[0m 47s] Epoch 48 [12800/13374] loss=0.00018645593110704794\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[0m 48s] Epoch 49 [2560/13374] loss=0.00015890528738964348\n",
      "[0m 48s] Epoch 49 [5120/13374] loss=0.00015936436066112946\n",
      "[0m 48s] Epoch 49 [7680/13374] loss=0.00016982038844920073\n",
      "[0m 48s] Epoch 49 [10240/13374] loss=0.00018045875440293458\n",
      "[0m 48s] Epoch 49 [12800/13374] loss=0.00018057471592328512\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[0m 49s] Epoch 50 [2560/13374] loss=0.00014834076137049124\n",
      "[0m 49s] Epoch 50 [5120/13374] loss=0.00015146486221055965\n",
      "[0m 49s] Epoch 50 [7680/13374] loss=0.00016475970284470046\n",
      "[0m 49s] Epoch 50 [10240/13374] loss=0.00017027721733029467\n",
      "[0m 49s] Epoch 50 [12800/13374] loss=0.00018250373395858332\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[0m 50s] Epoch 51 [2560/13374] loss=0.00014517665185849183\n",
      "[0m 50s] Epoch 51 [5120/13374] loss=0.00015799860448169057\n",
      "[0m 50s] Epoch 51 [7680/13374] loss=0.0001714292545026789\n",
      "[0m 50s] Epoch 51 [10240/13374] loss=0.0001788907484296942\n",
      "[0m 50s] Epoch 51 [12800/13374] loss=0.00018092257902026178\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5630/6700 84.03%\n",
      "[0m 51s] Epoch 52 [2560/13374] loss=0.0001573603760334663\n",
      "[0m 51s] Epoch 52 [5120/13374] loss=0.00016122549823194277\n",
      "[0m 51s] Epoch 52 [7680/13374] loss=0.00017210703687548328\n",
      "[0m 51s] Epoch 52 [10240/13374] loss=0.0001843626447225688\n",
      "[0m 51s] Epoch 52 [12800/13374] loss=0.00018679044063901528\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[0m 52s] Epoch 53 [2560/13374] loss=0.00014947831150493584\n",
      "[0m 52s] Epoch 53 [5120/13374] loss=0.0001632193445402663\n",
      "[0m 52s] Epoch 53 [7680/13374] loss=0.00016915443896626433\n",
      "[0m 52s] Epoch 53 [10240/13374] loss=0.0001746825764712412\n",
      "[0m 52s] Epoch 53 [12800/13374] loss=0.00018097417836543172\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5625/6700 83.96%\n",
      "[0m 53s] Epoch 54 [2560/13374] loss=0.00014532465211232194\n",
      "[0m 53s] Epoch 54 [5120/13374] loss=0.00016315377506543883\n",
      "[0m 53s] Epoch 54 [7680/13374] loss=0.00016960612847469748\n",
      "[0m 53s] Epoch 54 [10240/13374] loss=0.0001728311963233864\n",
      "[0m 53s] Epoch 54 [12800/13374] loss=0.00017875930876471102\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[0m 54s] Epoch 55 [2560/13374] loss=0.0001446747686713934\n",
      "[0m 54s] Epoch 55 [5120/13374] loss=0.00016782033490017056\n",
      "[0m 54s] Epoch 55 [7680/13374] loss=0.00016390118034905753\n",
      "[0m 54s] Epoch 55 [10240/13374] loss=0.00017176716846734053\n",
      "[0m 54s] Epoch 55 [12800/13374] loss=0.00017622266444959676\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[0m 55s] Epoch 56 [2560/13374] loss=0.00015177250097622163\n",
      "[0m 55s] Epoch 56 [5120/13374] loss=0.00015302531428460498\n",
      "[0m 55s] Epoch 56 [7680/13374] loss=0.00015534697134474604\n",
      "[0m 55s] Epoch 56 [10240/13374] loss=0.0001609369930520188\n",
      "[0m 55s] Epoch 56 [12800/13374] loss=0.00016936118292505852\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5639/6700 84.16%\n",
      "[0m 56s] Epoch 57 [2560/13374] loss=0.00012979755993001164\n",
      "[0m 56s] Epoch 57 [5120/13374] loss=0.00013853789314453025\n",
      "[0m 56s] Epoch 57 [7680/13374] loss=0.00014551261241043298\n",
      "[0m 56s] Epoch 57 [10240/13374] loss=0.00015559746552753495\n",
      "[0m 56s] Epoch 57 [12800/13374] loss=0.00017065074425772763\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[0m 57s] Epoch 58 [2560/13374] loss=0.0001676140498602763\n",
      "[0m 57s] Epoch 58 [5120/13374] loss=0.00017306205118075013\n",
      "[0m 57s] Epoch 58 [7680/13374] loss=0.00017213946096793128\n",
      "[0m 57s] Epoch 58 [10240/13374] loss=0.0001721969862046535\n",
      "[0m 57s] Epoch 58 [12800/13374] loss=0.00017428154387744144\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5640/6700 84.18%\n",
      "[0m 58s] Epoch 59 [2560/13374] loss=0.00016653786005917936\n",
      "[0m 58s] Epoch 59 [5120/13374] loss=0.00016285956589854323\n",
      "[0m 58s] Epoch 59 [7680/13374] loss=0.00016140557781909592\n",
      "[0m 58s] Epoch 59 [10240/13374] loss=0.00016622578696114942\n",
      "[0m 58s] Epoch 59 [12800/13374] loss=0.00017430476495064795\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5629/6700 84.01%\n",
      "[0m 59s] Epoch 60 [2560/13374] loss=0.00012102074615540914\n",
      "[0m 59s] Epoch 60 [5120/13374] loss=0.00014544876212312373\n",
      "[0m 59s] Epoch 60 [7680/13374] loss=0.0001571583682865215\n",
      "[0m 59s] Epoch 60 [10240/13374] loss=0.00017493341601948487\n",
      "[0m 59s] Epoch 60 [12800/13374] loss=0.00017842295084847138\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[1m 0s] Epoch 61 [2560/13374] loss=0.000198833742615534\n",
      "[1m 0s] Epoch 61 [5120/13374] loss=0.0001810047950129956\n",
      "[1m 0s] Epoch 61 [7680/13374] loss=0.000176410020988745\n",
      "[1m 0s] Epoch 61 [10240/13374] loss=0.00018343752053624486\n",
      "[1m 0s] Epoch 61 [12800/13374] loss=0.00019121519493637607\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[1m 1s] Epoch 62 [2560/13374] loss=0.00022985020041232929\n",
      "[1m 1s] Epoch 62 [5120/13374] loss=0.00024037848343141376\n",
      "[1m 1s] Epoch 62 [7680/13374] loss=0.0002376424507626022\n",
      "[1m 1s] Epoch 62 [10240/13374] loss=0.00023770797924953512\n",
      "[1m 1s] Epoch 62 [12800/13374] loss=0.00023777670139679685\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[1m 2s] Epoch 63 [2560/13374] loss=0.00018371271726209671\n",
      "[1m 2s] Epoch 63 [5120/13374] loss=0.00018027768455795013\n",
      "[1m 2s] Epoch 63 [7680/13374] loss=0.0001915364841503712\n",
      "[1m 2s] Epoch 63 [10240/13374] loss=0.00019325778430356877\n",
      "[1m 2s] Epoch 63 [12800/13374] loss=0.0001988413798972033\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[1m 3s] Epoch 64 [2560/13374] loss=0.00014337025131681003\n",
      "[1m 3s] Epoch 64 [5120/13374] loss=0.0001572690685861744\n",
      "[1m 3s] Epoch 64 [7680/13374] loss=0.00015781316372643535\n",
      "[1m 3s] Epoch 64 [10240/13374] loss=0.0001658470382608357\n",
      "[1m 3s] Epoch 64 [12800/13374] loss=0.00017952103356947191\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[1m 4s] Epoch 65 [2560/13374] loss=0.0001168813971162308\n",
      "[1m 4s] Epoch 65 [5120/13374] loss=0.00013775352526863572\n",
      "[1m 4s] Epoch 65 [7680/13374] loss=0.00015191100416510987\n",
      "[1m 4s] Epoch 65 [10240/13374] loss=0.00016083889222500146\n",
      "[1m 4s] Epoch 65 [12800/13374] loss=0.00016989462223136796\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[1m 5s] Epoch 66 [2560/13374] loss=0.00011994239248451777\n",
      "[1m 5s] Epoch 66 [5120/13374] loss=0.00014027795659785625\n",
      "[1m 5s] Epoch 66 [7680/13374] loss=0.00014978871113271452\n",
      "[1m 5s] Epoch 66 [10240/13374] loss=0.0001613175247257459\n",
      "[1m 5s] Epoch 66 [12800/13374] loss=0.00016562175427679903\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5632/6700 84.06%\n",
      "[1m 6s] Epoch 67 [2560/13374] loss=0.00013299363490659744\n",
      "[1m 6s] Epoch 67 [5120/13374] loss=0.00015544994857918937\n",
      "[1m 6s] Epoch 67 [7680/13374] loss=0.00015188564890801596\n",
      "[1m 6s] Epoch 67 [10240/13374] loss=0.00016026912653614999\n",
      "[1m 6s] Epoch 67 [12800/13374] loss=0.00016468099609483035\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[1m 7s] Epoch 68 [2560/13374] loss=0.0001540470591862686\n",
      "[1m 7s] Epoch 68 [5120/13374] loss=0.00014953142454032786\n",
      "[1m 7s] Epoch 68 [7680/13374] loss=0.0001647679416540389\n",
      "[1m 7s] Epoch 68 [10240/13374] loss=0.0001608278984349454\n",
      "[1m 7s] Epoch 68 [12800/13374] loss=0.00016017924222978765\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5630/6700 84.03%\n",
      "[1m 8s] Epoch 69 [2560/13374] loss=0.000130108161829412\n",
      "[1m 8s] Epoch 69 [5120/13374] loss=0.00014970021911722142\n",
      "[1m 8s] Epoch 69 [7680/13374] loss=0.0001564976385755775\n",
      "[1m 8s] Epoch 69 [10240/13374] loss=0.0001459019447793253\n",
      "[1m 8s] Epoch 69 [12800/13374] loss=0.00016436097561381758\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[1m 9s] Epoch 70 [2560/13374] loss=0.0001405881135724485\n",
      "[1m 9s] Epoch 70 [5120/13374] loss=0.00014531969536619728\n",
      "[1m 9s] Epoch 70 [7680/13374] loss=0.00015759922171127984\n",
      "[1m 9s] Epoch 70 [10240/13374] loss=0.00015900034068181413\n",
      "[1m 9s] Epoch 70 [12800/13374] loss=0.0001627055105927866\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5641/6700 84.19%\n",
      "[1m 10s] Epoch 71 [2560/13374] loss=0.00013604419509647415\n",
      "[1m 10s] Epoch 71 [5120/13374] loss=0.00013168357945687604\n",
      "[1m 10s] Epoch 71 [7680/13374] loss=0.00014040258271658484\n",
      "[1m 10s] Epoch 71 [10240/13374] loss=0.00014994320772530046\n",
      "[1m 10s] Epoch 71 [12800/13374] loss=0.00015765869378810748\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[1m 11s] Epoch 72 [2560/13374] loss=0.00014466360735241324\n",
      "[1m 11s] Epoch 72 [5120/13374] loss=0.0001496147920988733\n",
      "[1m 11s] Epoch 72 [7680/13374] loss=0.0001497590717917774\n",
      "[1m 11s] Epoch 72 [10240/13374] loss=0.0001556599809191539\n",
      "[1m 11s] Epoch 72 [12800/13374] loss=0.0001602194107545074\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[1m 12s] Epoch 73 [2560/13374] loss=0.00013900813282816671\n",
      "[1m 12s] Epoch 73 [5120/13374] loss=0.00014513558526232372\n",
      "[1m 12s] Epoch 73 [7680/13374] loss=0.0001455510680292112\n",
      "[1m 12s] Epoch 73 [10240/13374] loss=0.0001547212623336236\n",
      "[1m 12s] Epoch 73 [12800/13374] loss=0.00016016780951758847\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5641/6700 84.19%\n",
      "[1m 13s] Epoch 74 [2560/13374] loss=0.00013305818065418863\n",
      "[1m 13s] Epoch 74 [5120/13374] loss=0.00013563958236773033\n",
      "[1m 13s] Epoch 74 [7680/13374] loss=0.00015124611769958088\n",
      "[1m 13s] Epoch 74 [10240/13374] loss=0.00015521123441430973\n",
      "[1m 13s] Epoch 74 [12800/13374] loss=0.0001586200298334006\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[1m 14s] Epoch 75 [2560/13374] loss=0.00015354831630247644\n",
      "[1m 14s] Epoch 75 [5120/13374] loss=0.00014895773674652446\n",
      "[1m 14s] Epoch 75 [7680/13374] loss=0.00015466092372662388\n",
      "[1m 14s] Epoch 75 [10240/13374] loss=0.00014791005378356203\n",
      "[1m 15s] Epoch 75 [12800/13374] loss=0.00015603566411300562\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[1m 15s] Epoch 76 [2560/13374] loss=0.0001398371663526632\n",
      "[1m 15s] Epoch 76 [5120/13374] loss=0.00013450406440824737\n",
      "[1m 15s] Epoch 76 [7680/13374] loss=0.0001434037352737505\n",
      "[1m 15s] Epoch 76 [10240/13374] loss=0.0001484948064899072\n",
      "[1m 16s] Epoch 76 [12800/13374] loss=0.00015754489068058318\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5621/6700 83.90%\n",
      "[1m 16s] Epoch 77 [2560/13374] loss=0.00010889456461882219\n",
      "[1m 16s] Epoch 77 [5120/13374] loss=0.00013552697928389534\n",
      "[1m 16s] Epoch 77 [7680/13374] loss=0.0001363323171972297\n",
      "[1m 16s] Epoch 77 [10240/13374] loss=0.00014599543410440675\n",
      "[1m 17s] Epoch 77 [12800/13374] loss=0.00015848617345909588\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5638/6700 84.15%\n",
      "[1m 17s] Epoch 78 [2560/13374] loss=9.778489038581029e-05\n",
      "[1m 17s] Epoch 78 [5120/13374] loss=0.00012234119712957182\n",
      "[1m 17s] Epoch 78 [7680/13374] loss=0.00013654309247309964\n",
      "[1m 17s] Epoch 78 [10240/13374] loss=0.00014184172032400967\n",
      "[1m 18s] Epoch 78 [12800/13374] loss=0.00015516734303673728\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5644/6700 84.24%\n",
      "[1m 18s] Epoch 79 [2560/13374] loss=0.00011608726490521803\n",
      "[1m 18s] Epoch 79 [5120/13374] loss=0.0001315392910328228\n",
      "[1m 18s] Epoch 79 [7680/13374] loss=0.00013897229776678918\n",
      "[1m 18s] Epoch 79 [10240/13374] loss=0.00014784109080210329\n",
      "[1m 19s] Epoch 79 [12800/13374] loss=0.00015309893497033044\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5628/6700 84.00%\n",
      "[1m 19s] Epoch 80 [2560/13374] loss=0.00011623586105997674\n",
      "[1m 19s] Epoch 80 [5120/13374] loss=0.00013302890765771737\n",
      "[1m 19s] Epoch 80 [7680/13374] loss=0.00012912973422013845\n",
      "[1m 19s] Epoch 80 [10240/13374] loss=0.00014186570333549753\n",
      "[1m 20s] Epoch 80 [12800/13374] loss=0.00015696399568696507\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[1m 20s] Epoch 81 [2560/13374] loss=0.00011773098740377463\n",
      "[1m 20s] Epoch 81 [5120/13374] loss=0.0001227446959092049\n",
      "[1m 20s] Epoch 81 [7680/13374] loss=0.00012443192026694304\n",
      "[1m 20s] Epoch 81 [10240/13374] loss=0.00013253615961730247\n",
      "[1m 21s] Epoch 81 [12800/13374] loss=0.00014609290505177342\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5645/6700 84.25%\n",
      "[1m 21s] Epoch 82 [2560/13374] loss=0.00012624068040167913\n",
      "[1m 21s] Epoch 82 [5120/13374] loss=0.00013657861927640624\n",
      "[1m 21s] Epoch 82 [7680/13374] loss=0.0001461059429857414\n",
      "[1m 21s] Epoch 82 [10240/13374] loss=0.00014802888999838616\n",
      "[1m 22s] Epoch 82 [12800/13374] loss=0.00015403187790070661\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[1m 22s] Epoch 83 [2560/13374] loss=9.601403362466954e-05\n",
      "[1m 22s] Epoch 83 [5120/13374] loss=0.00011559446720639243\n",
      "[1m 22s] Epoch 83 [7680/13374] loss=0.0001346885910606943\n",
      "[1m 22s] Epoch 83 [10240/13374] loss=0.00014408427705348004\n",
      "[1m 23s] Epoch 83 [12800/13374] loss=0.00015312932213419118\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[1m 23s] Epoch 84 [2560/13374] loss=0.00014379825734067708\n",
      "[1m 23s] Epoch 84 [5120/13374] loss=0.0001406092953402549\n",
      "[1m 23s] Epoch 84 [7680/13374] loss=0.00015105147128148625\n",
      "[1m 23s] Epoch 84 [10240/13374] loss=0.00015658097363484557\n",
      "[1m 24s] Epoch 84 [12800/13374] loss=0.0001540511161147151\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[1m 24s] Epoch 85 [2560/13374] loss=0.00012320289679337292\n",
      "[1m 24s] Epoch 85 [5120/13374] loss=0.0001308879884163616\n",
      "[1m 24s] Epoch 85 [7680/13374] loss=0.0001428912236103012\n",
      "[1m 24s] Epoch 85 [10240/13374] loss=0.00014208276606950677\n",
      "[1m 25s] Epoch 85 [12800/13374] loss=0.00014884989985148423\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[1m 25s] Epoch 86 [2560/13374] loss=0.0001390170829836279\n",
      "[1m 25s] Epoch 86 [5120/13374] loss=0.00014059739514777904\n",
      "[1m 25s] Epoch 86 [7680/13374] loss=0.00013994219552841968\n",
      "[1m 25s] Epoch 86 [10240/13374] loss=0.00014107806418905966\n",
      "[1m 26s] Epoch 86 [12800/13374] loss=0.0001505196641664952\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[1m 26s] Epoch 87 [2560/13374] loss=0.0001344283424259629\n",
      "[1m 26s] Epoch 87 [5120/13374] loss=0.00012540834723040462\n",
      "[1m 26s] Epoch 87 [7680/13374] loss=0.00013295674386123817\n",
      "[1m 26s] Epoch 87 [10240/13374] loss=0.00014284736789704766\n",
      "[1m 27s] Epoch 87 [12800/13374] loss=0.0001478749119269196\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[1m 27s] Epoch 88 [2560/13374] loss=0.0001324082986684516\n",
      "[1m 27s] Epoch 88 [5120/13374] loss=0.00012862259354733395\n",
      "[1m 27s] Epoch 88 [7680/13374] loss=0.00013109774930247415\n",
      "[1m 27s] Epoch 88 [10240/13374] loss=0.00014083395562920488\n",
      "[1m 28s] Epoch 88 [12800/13374] loss=0.00014900714173563757\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5634/6700 84.09%\n",
      "[1m 28s] Epoch 89 [2560/13374] loss=0.00012251887310412712\n",
      "[1m 28s] Epoch 89 [5120/13374] loss=0.0001318914168223273\n",
      "[1m 28s] Epoch 89 [7680/13374] loss=0.00013916814471789016\n",
      "[1m 28s] Epoch 89 [10240/13374] loss=0.00014509328702843048\n",
      "[1m 29s] Epoch 89 [12800/13374] loss=0.00015303836305974983\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5628/6700 84.00%\n",
      "[1m 29s] Epoch 90 [2560/13374] loss=0.0001132663048338145\n",
      "[1m 29s] Epoch 90 [5120/13374] loss=0.00011696168985508848\n",
      "[1m 29s] Epoch 90 [7680/13374] loss=0.00012046538880288911\n",
      "[1m 29s] Epoch 90 [10240/13374] loss=0.0001280069400309003\n",
      "[1m 30s] Epoch 90 [12800/13374] loss=0.00014429465416469612\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[1m 30s] Epoch 91 [2560/13374] loss=0.00010213008572463877\n",
      "[1m 30s] Epoch 91 [5120/13374] loss=0.00011889847301063127\n",
      "[1m 30s] Epoch 91 [7680/13374] loss=0.000126158342754934\n",
      "[1m 30s] Epoch 91 [10240/13374] loss=0.00013724393138545566\n",
      "[1m 31s] Epoch 91 [12800/13374] loss=0.00014950607233913616\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5636/6700 84.12%\n",
      "[1m 31s] Epoch 92 [2560/13374] loss=0.00011223137335036881\n",
      "[1m 31s] Epoch 92 [5120/13374] loss=0.00012609823061211501\n",
      "[1m 31s] Epoch 92 [7680/13374] loss=0.0001429563630760337\n",
      "[1m 31s] Epoch 92 [10240/13374] loss=0.00014809385356784333\n",
      "[1m 32s] Epoch 92 [12800/13374] loss=0.00015027345856651663\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[1m 32s] Epoch 93 [2560/13374] loss=0.00011175937179359607\n",
      "[1m 32s] Epoch 93 [5120/13374] loss=0.00012027142693114002\n",
      "[1m 32s] Epoch 93 [7680/13374] loss=0.00013260970566382943\n",
      "[1m 32s] Epoch 93 [10240/13374] loss=0.0001406029230565764\n",
      "[1m 33s] Epoch 93 [12800/13374] loss=0.00014693949051434175\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[1m 33s] Epoch 94 [2560/13374] loss=0.00013874377982574514\n",
      "[1m 33s] Epoch 94 [5120/13374] loss=0.00012912488309666515\n",
      "[1m 33s] Epoch 94 [7680/13374] loss=0.00014128563586079205\n",
      "[1m 33s] Epoch 94 [10240/13374] loss=0.00014571725769201293\n",
      "[1m 34s] Epoch 94 [12800/13374] loss=0.0001478068475262262\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[1m 34s] Epoch 95 [2560/13374] loss=0.00014522725468850694\n",
      "[1m 34s] Epoch 95 [5120/13374] loss=0.00013318268865987194\n",
      "[1m 34s] Epoch 95 [7680/13374] loss=0.00014101996227206353\n",
      "[1m 34s] Epoch 95 [10240/13374] loss=0.00014576311532437102\n",
      "[1m 35s] Epoch 95 [12800/13374] loss=0.00014673581856186502\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[1m 35s] Epoch 96 [2560/13374] loss=0.00014798463889746926\n",
      "[1m 35s] Epoch 96 [5120/13374] loss=0.00013008903879381252\n",
      "[1m 35s] Epoch 96 [7680/13374] loss=0.00013981599501372937\n",
      "[1m 35s] Epoch 96 [10240/13374] loss=0.00014711771000293085\n",
      "[1m 36s] Epoch 96 [12800/13374] loss=0.00015081697798450478\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5626/6700 83.97%\n",
      "[1m 36s] Epoch 97 [2560/13374] loss=0.00011517391030793078\n",
      "[1m 36s] Epoch 97 [5120/13374] loss=0.0001368978206301108\n",
      "[1m 36s] Epoch 97 [7680/13374] loss=0.00014563064542016946\n",
      "[1m 36s] Epoch 97 [10240/13374] loss=0.00014485657902696403\n",
      "[1m 37s] Epoch 97 [12800/13374] loss=0.00014959122534492054\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5606/6700 83.67%\n",
      "[1m 37s] Epoch 98 [2560/13374] loss=0.00011743027716875077\n",
      "[1m 37s] Epoch 98 [5120/13374] loss=0.000129615435434971\n",
      "[1m 37s] Epoch 98 [7680/13374] loss=0.00012822744853716964\n",
      "[1m 37s] Epoch 98 [10240/13374] loss=0.00014413975623028819\n",
      "[1m 38s] Epoch 98 [12800/13374] loss=0.00014752414150279947\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5626/6700 83.97%\n",
      "[1m 38s] Epoch 99 [2560/13374] loss=0.0001538023803732358\n",
      "[1m 38s] Epoch 99 [5120/13374] loss=0.00014951426564948633\n",
      "[1m 38s] Epoch 99 [7680/13374] loss=0.00014230215965653769\n",
      "[1m 38s] Epoch 99 [10240/13374] loss=0.00014815395861660362\n",
      "[1m 39s] Epoch 99 [12800/13374] loss=0.0001457584966556169\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[1m 39s] Epoch 100 [2560/13374] loss=0.00011818405910162255\n",
      "[1m 39s] Epoch 100 [5120/13374] loss=0.0001393651225953363\n",
      "[1m 39s] Epoch 100 [7680/13374] loss=0.0001415951737726573\n",
      "[1m 39s] Epoch 100 [10240/13374] loss=0.00014384919941221597\n",
      "[1m 40s] Epoch 100 [12800/13374] loss=0.00015155326065723784\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    # 是否使用GPU\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "    # 构造损失函数和优化器\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    start = time.time() \n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS) \n",
    "    acc_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle，训练和测试封装在不同的函数中\n",
    "        trainModel() \n",
    "        acc = testModel() \n",
    "        acc_list.append(acc)\n",
    "        epoch_list.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_list = np.array(acc_list) \n",
    "plt.plot(epoch_list, acc_list) \n",
    "plt.xlabel('Epoch') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中看最好的训练轮数在20左右"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
