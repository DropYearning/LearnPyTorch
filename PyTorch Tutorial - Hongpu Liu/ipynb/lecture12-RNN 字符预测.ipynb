{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 循环神经网络——字符预测\n",
    "## 学习用RNN来将一个序列转换为另一个序列\n",
    "![qfUyds](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/qfUyds.png)\n",
    "![k8SRpV](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/k8SRpV.png)\n",
    "![3bn5e2](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/3bn5e2.png)\n",
    "- 最终可以看成是多分类问题，因此可以使用交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 使用RNN Cell来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "input_size = 4  # input_size 输入特征维数\n",
    "hidden_size = 4 # hidden_size 隐层状态的维数\n",
    "num_layers = 1 # num_layers RNN层的个数，在图中竖向的是层数，横向的是seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "tensor([[[0., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.]]])\n",
      "torch.Size([5, 1, 4])\n",
      "tensor([[3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2]])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol\n",
    "one_hot_lookup = [[1, 0, 0, 0], \n",
    "                  [0, 1, 0, 0], \n",
    "                  [0, 0, 1, 0], \n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # 将x_data转为独热向量\n",
    "# Reshape the inputs to （𝒔𝒆𝒒𝑳𝒆𝒏, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆）\n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size) \n",
    "labels = torch.LongTensor(y_data).view(-1, 1) # Reshape the labels to（𝒔𝒆𝒒𝑳𝒆𝒏, 𝟏）\n",
    "print(x_one_hot)\n",
    "print(inputs)\n",
    "print(inputs.size())\n",
    "print(labels)\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Model\n",
    "class RNN_Cell_Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(RNN_Cell_Model, self).__init__() \n",
    "        # Initial the parameters\n",
    "        self.batch_size = batch_size \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnncell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnncell(input, hidden) # h_t = cell(x_t , h_t-1)\n",
    "        return hidden\n",
    "\n",
    "    def init_hidden(self): # 生成默认的初始h0\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "net1 = RNN_Cell_Model(input_size, hidden_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(net1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string: eeeee, Epoch [1/15] loss=8.7338\n",
      "Predicted string: helll, Epoch [2/15] loss=6.8109\n",
      "Predicted string: hllll, Epoch [3/15] loss=5.7981\n",
      "Predicted string: hhlll, Epoch [4/15] loss=5.2475\n",
      "Predicted string: hhlll, Epoch [5/15] loss=4.9066\n",
      "Predicted string: hhlll, Epoch [6/15] loss=4.6345\n",
      "Predicted string: hhlll, Epoch [7/15] loss=4.3896\n",
      "Predicted string: hhlll, Epoch [8/15] loss=4.1650\n",
      "Predicted string: hhlll, Epoch [9/15] loss=3.9674\n",
      "Predicted string: hhlll, Epoch [10/15] loss=3.7972\n",
      "Predicted string: hhlll, Epoch [11/15] loss=3.6606\n",
      "Predicted string: ohlll, Epoch [12/15] loss=3.5416\n",
      "Predicted string: ohlll, Epoch [13/15] loss=3.4050\n",
      "Predicted string: ohlll, Epoch [14/15] loss=3.2450\n",
      "Predicted string: ohlol, Epoch [15/15] loss=3.0918\n"
     ]
    }
   ],
   "source": [
    "## 使用RNN Cell训练\n",
    "for epoch in range(15):\n",
    "    loss = 0 \n",
    "    optimizer.zero_grad() \n",
    "    hidden = net1.init_hidden() # 初始化h0\n",
    "    print('Predicted string: ', end='') \n",
    "    # Shape of inputs :(𝒔𝒆𝒒𝑳𝒆𝒏, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆)\n",
    "    # Shape of input:(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆)\n",
    "    # Shape of labels :(𝒔𝒆𝒒𝑺𝒊𝒛𝒆, 𝟏)\n",
    "    # Shape of label:(𝟏)\n",
    "    for input, label in zip(inputs, labels):\n",
    "        # Training steps\n",
    "        hidden = net1(input, hidden)\n",
    "        loss += criterion(hidden, label)\n",
    "        _, idx = hidden.max(dim=1) # hidden.max表示多分类的最大项值（即预测结果）\n",
    "        print(idx2char[idx.item()], end='') \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    print(', Epoch [%d/15] loss=%.4f' % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 使用RNN Module训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0], \n",
    "                  [0, 1, 0, 0], \n",
    "                  [0, 0, 1, 0], \n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # 将x_data转为独热向量\n",
    "\n",
    "# Reshape the inputs to （𝒔𝒆𝒒𝑳𝒆𝒏, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, hiddenSize)\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size) \n",
    "# Shape of labels :(𝒔𝒆𝒒𝑳𝒆𝒏 × 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝟏)\n",
    "labels = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(RNN_Model, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.batch_size = batch_size \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = torch.nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers)\n",
    "    def forward(self, input):\n",
    "        # Shape of hidden :(𝒏𝒖𝒎𝑳𝒂𝒚𝒆𝒓𝒔, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆)\n",
    "        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size) \n",
    "        out, _ = self.rnn(input, hidden) \n",
    "        # Reshape out to:(𝒔𝒆𝒒𝑳𝒆𝒏 × 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆)\n",
    "        return out.view(-1, self.hidden_size)\n",
    "net2 = RNN_Model(input_size, hidden_size, batch_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2 = torch.nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(net2.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  ololl, Epoch [1/15] loss=1.1037\n",
      "Predicted:  ololl, Epoch [2/15] loss=1.0424\n",
      "Predicted:  ooool, Epoch [3/15] loss=0.9988\n",
      "Predicted:  oooll, Epoch [4/15] loss=0.9634\n",
      "Predicted:  oooll, Epoch [5/15] loss=0.9365\n",
      "Predicted:  oholl, Epoch [6/15] loss=0.9179\n",
      "Predicted:  ohlll, Epoch [7/15] loss=0.9063\n",
      "Predicted:  ohlll, Epoch [8/15] loss=0.8977\n",
      "Predicted:  ohlll, Epoch [9/15] loss=0.8866\n",
      "Predicted:  ohlll, Epoch [10/15] loss=0.8693\n",
      "Predicted:  ohlll, Epoch [11/15] loss=0.8443\n",
      "Predicted:  ohlll, Epoch [12/15] loss=0.8124\n",
      "Predicted:  ohlll, Epoch [13/15] loss=0.7766\n",
      "Predicted:  ohlll, Epoch [14/15] loss=0.7414\n",
      "Predicted:  ohlll, Epoch [15/15] loss=0.7126\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    optimizer.zero_grad() \n",
    "    outputs = net2(inputs)\n",
    "    loss = criterion2(outputs, labels)\n",
    "    loss.backward() \n",
    "    optimizer2.step() \n",
    "    \n",
    "    _, idx = outputs.max(dim=1) \n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/15] loss=%.4f' % (epoch+1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
