{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 循环神经网络——字符预测\n",
    "## 使用嵌入层代替独热编码\n",
    "![dmwb6Q](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/dmwb6Q.png)\n",
    "![s9VCVx](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/s9VCVx.png)\n",
    "![MQHFCF](https://gitee.com/pxqp9W/testmarkdown/raw/master/imgs/2020/07/MQHFCF.png)\n",
    "- [万物皆Embedding，从经典的word2vec到深度学习基本操作item2vec - 知乎](https://zhuanlan.zhihu.com/p/53194407)\n",
    "- [Embedding 的理解 - 知乎](https://zhuanlan.zhihu.com/p/46016518)\n",
    "- [深度学习中的embedding_星辰大海，脚踏实地-CSDN博客_embedding](https://blog.csdn.net/qq_35799003/article/details/84780289)\n",
    "- [深度学习中 Embedding层两大作用的个人理解_weixin_42078618的博客-CSDN博客_embedding层](https://blog.csdn.net/weixin_42078618/article/details/82999906)\n",
    "\n",
    "## 嵌入层的理解\n",
    "- 降维作用：\n",
    "    - 假设：我们有一个2 x 6的矩阵，然后乘上一个6 x 3的矩阵后，变成了一个2 x 3的矩阵 -> 把一个12个元素的矩阵变成6个元素的矩阵\n",
    "    - 假如我们有一个100W X10W的矩阵，用它乘上一个10W X 20的矩阵，我们可以把它降到100W X 20，瞬间量级降了10W/20=5000倍\n",
    "    - 这就是嵌入层的一个作用——降维。中间那个 10W X 20的矩阵，可以理解为**查询表**，也可以理解为**映射表**，也可以理解为**过度表**\n",
    "- 既然可以降维，当然也可以升维。\n",
    "    - embedding的又一个作用体现了：对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了。\n",
    "    - 同时，这个embedding是一直在学习在优化的，就使得整个拉近拉远的过程慢慢形成一个良好的观察点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_class = 4 \n",
    "input_size = 4 \n",
    "hidden_size = 8 \n",
    "embedding_size = 10\n",
    "num_layers = 2 \n",
    "batch_size = 1 \n",
    "seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [[1, 0, 2, 2, 3]] # hello, 注意这里的shape\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol\n",
    "\n",
    "inputs = torch.LongTensor(x_data) \n",
    "labels = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__() \n",
    "        # Lookup matrix of Embedding:（𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆, 𝒆𝒎𝒃𝒆𝒅𝒅𝒊𝒏𝒈𝑺𝒊𝒛𝒆）\n",
    "        self.emb = torch.nn.Embedding(input_size, embedding_size) \n",
    "        # Input of RNN:（𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒆𝒎𝒃𝒆𝒅𝒅𝒊𝒏𝒈𝑺𝒊𝒛𝒆）\n",
    "        # Output of RNN:（𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆）\n",
    "        self.rnn = torch.nn.RNN(input_size=embedding_size, \n",
    "                                hidden_size=hidden_size, \n",
    "                                num_layers=num_layers, \n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size( 0), hidden_size) \n",
    "        x = self.emb(x) # (batch, seqLen, embeddingSize) \n",
    "        x, _ = self.rnn(x, hidden) \n",
    "        # Input of FC Layer: （𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆）\n",
    "        x = self.fc(x) \n",
    "        # Output of FC Layer：（𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒏𝒖𝒎𝑪𝒍𝒂𝒔𝒔）\n",
    "        return x.view(-1, num_class) # Reshape result to use Cross Entropy\n",
    "\n",
    "net = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  hhhhh, Epoch [1/15] loss = 1.398\n",
      "Predicted:  loolo, Epoch [2/15] loss = 2.079\n",
      "Predicted:  olllh, Epoch [3/15] loss = 1.616\n",
      "Predicted:  ohloo, Epoch [4/15] loss = 0.700\n",
      "Predicted:  ohool, Epoch [5/15] loss = 0.881\n",
      "Predicted:  ohool, Epoch [6/15] loss = 0.586\n",
      "Predicted:  lhlll, Epoch [7/15] loss = 0.583\n",
      "Predicted:  lhlll, Epoch [8/15] loss = 0.557\n",
      "Predicted:  ohlol, Epoch [9/15] loss = 0.176\n",
      "Predicted:  ohlol, Epoch [10/15] loss = 0.037\n",
      "Predicted:  ohlol, Epoch [11/15] loss = 0.021\n",
      "Predicted:  ohlol, Epoch [12/15] loss = 0.018\n",
      "Predicted:  ohlol, Epoch [13/15] loss = 0.015\n",
      "Predicted:  ohlol, Epoch [14/15] loss = 0.012\n",
      "Predicted:  ohlol, Epoch [15/15] loss = 0.013\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    optimizer.zero_grad() \n",
    "    outputs = net(inputs) \n",
    "    loss = criterion(outputs, labels) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    _, idx = outputs.max(dim=1) \n",
    "    idx = idx.data.numpy() \n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='') \n",
    "    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
